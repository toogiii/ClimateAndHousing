{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1 Development Notebook: Climate and Housing\n",
    "\n",
    "This file contains development code for project 1 for ECON 1680. It contains all code required to replicate the results outlined in the draft. Inline citations are included for all code segments that are not original. The code first cleans the data, generates some descriptive statistics, and then leverages the methodology discussed in the paper: \n",
    "We use different regressions and dimension reduction techniques to analyze the relationship between climate risk and housing prices, using space as our source of variation.\n",
    "\n",
    "## 1. Imports and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dateutil.parser import parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_date(string):\n",
    "    # From https://stackoverflow.com/questions/25341945/check-if-string-has-date-any-format\n",
    "    #   Checks if a string can be interpreted as a date\n",
    "    try:\n",
    "        parse(string)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pct_change(row, start_date, end_date):\n",
    "    # This function gets the percentage change in ZHVI from 2000-2019 for a given county.\n",
    "    #   We use this later to generate one of our dependent variables.\n",
    "    pct_change = (row[end_date] - row[start_date]) / row[start_date]\n",
    "    return pct_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_volatility(row, date_vars):\n",
    "    # This function average of the yearly coefficients of variation in ZHVI in a county.\n",
    "    coef_of_vars = []\n",
    "    for n in range(0, len(date_vars), 12):\n",
    "        year_vars = date_vars[n:n + 12]\n",
    "        year_price_data = row[year_vars].tolist()\n",
    "\n",
    "        # Drop year observations if we don't have sufficient data to calculate the coefficient of variation.\n",
    "        if True not in [np.isnan(i) for i in year_price_data]:\n",
    "            coef_of_vars.append(np.std(year_price_data) / np.mean(year_price_data))\n",
    "        \n",
    "    # If we have no information about volatility, return missing.\n",
    "    if coef_of_vars == []:\n",
    "        return np.nan\n",
    "    return np.mean(coef_of_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data: Cleaning and Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to production directory: note that you must include Data, Figures, and Code subdirectories.\n",
    "path = \"C:\\\\Users\\\\garvg\\\\Downloads\\\\Project 1\\\\ClimateAndHousing\\\\\"\n",
    "\n",
    "# Read in CSV data\n",
    "zillow = pd.read_csv(path + \"Data\\\\zillow.csv\")\n",
    "nri_fema = pd.read_csv(path + \"Data\\\\nri_fema.csv\")\n",
    "oi_covars = pd.read_csv(path + \"Data\\\\oi_covars.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge CSV data by State and County FIPS codes and save\n",
    "clim_hous_data = zillow.merge(nri_fema, on=[\"state_fips\", \"county_fips\"], validate=\"one_to_one\")\n",
    "all_data = clim_hous_data.merge(oi_covars, on=[\"state_fips\", \"county_fips\"], validate=\"one_to_one\")\n",
    "all_data.to_csv(path + \"Data\\\\all_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify columns to keep: hazard scores, controls, and house value\n",
    "hazard_vars = [\n",
    "    \"Avalanche\", \"Coastal Flooding\", \"Cold Wave\", \"Drought\", \"Earthquake\", \n",
    "    \"Hail\", \"Heat Wave\", \"Hurricane\", \"Ice Storm\", \"Landslide\", \"Lightning\",\n",
    "    \"Riverine Flooding\", \"Strong Wind\", \"Tornado\", \"Tsunami\", \"Volcanic Activity\",\n",
    "    \"Wildfire\", \"Winter Weather\"]\n",
    "hazard_vars = [hazard + \" - Hazard Type Risk Index Score\" for hazard in hazard_vars]\n",
    "hazard_vars.append(\"National Risk Index - Score - Composite\")\n",
    "\n",
    "housing_vars = [col for col in all_data.columns if is_date(col) and \"202\" not in col]\n",
    "housing_vars.sort()\n",
    "\n",
    "control_vars = [\n",
    "    \"Population (2020)\", \"Building Value ($)\", \"Agriculture Value ($)\", \"Area (sq mi)\", \"job_density_2013\",\n",
    "    \"ann_avg_job_growth_2004_2013\", \"ln_wage_growth_hs_grad\", \"emp2000\", \"foreign_share2010\", \n",
    "    \"mean_commutetime2000\", \"frac_coll_plus2000\", \"frac_coll_plus2010\", \"hhinc_mean2000\",\n",
    "    \"med_hhinc1990\", \"med_hhinc2016\", \"poor_share1990\", \"poor_share2000\", \"poor_share2010\",\n",
    "    \"share_white2000\", \"share_black2000\", \"share_hisp2000\", \"share_asian2000\",\n",
    "    \"share_white2010\", \"share_black2010\", \"share_hisp2010\", \"share_asian2010\"\n",
    "    ]\n",
    "\n",
    "columns = [\"State Name\", \"County Name\", \"state_fips\", \"county_fips\"]\n",
    "columns.extend(hazard_vars)\n",
    "columns.extend(housing_vars)\n",
    "columns.extend(control_vars)\n",
    "\n",
    "all_data_pruned = all_data[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\garvg\\AppData\\Local\\Temp\\ipykernel_13828\\2145004020.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  all_data_pruned[hazard] = all_data_pruned[hazard].fillna(0)\n"
     ]
    }
   ],
   "source": [
    "# For hazard data, rename variables and replace missing values with zeroes:\n",
    "#   Missing values imply that \"Community is not considered at risk for hazard type.\"\n",
    "#   according to the official NRI documentation here:\n",
    "#   https://www.fema.gov/sites/default/files/documents/fema_national-risk-index_technical-documentation.pdf\n",
    "hazard_rename_dict = dict()\n",
    "\n",
    "for hazard in hazard_vars:\n",
    "    hazard_rename_dict[hazard] = hazard[:hazard.index(\" - \")]\n",
    "    all_data_pruned[hazard] = all_data_pruned[hazard].fillna(0)\n",
    "\n",
    "all_data_pruned = all_data_pruned.rename(hazard_rename_dict, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependent Variable Generation\n",
    "\n",
    "The NRI takes into account historical loss data from 1996-2019 when computing natural hazard risk. We can thus take the NRI as a measure of both hazard risk and historical hazard loss. This can be supported by the documentation, which is included here: https://www.fema.gov/sites/default/files/documents/fema_national-risk-index_technical-documentation.pdf. This will be corroborated later, as we will show that the incidence of certain natural disasters in South Carolina is strongly correlated with these risk indices.\n",
    "\n",
    "We bring this up to explain that we will consider Zillow home value data (the Zillow Home Value Index, or ZHVI is our measure) from the years 2000 (earliest available) to 2019. Our dependent variables will thus be the following:\n",
    "1. Mean house value over this period (to capture the relationship between risk and magnitude of housing prices)\n",
    "2. Average percentage change in housing price (to capture the relationship between risk and long-term upward/downward trends)\n",
    "3. Average within-year variance in housing price (to capture the relationship between risk and short-term housing price volatility)\n",
    "\n",
    "We generate these values below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate mean ZHVI over 2000-2019\n",
    "all_data_pruned[\"ZHVI_mean\"] = all_data_pruned[housing_vars].mean(axis=1)\n",
    "\n",
    "# Generate mean percentage change over 2000-2019\n",
    "all_data_pruned[\"ZHVI_trend\"] = all_data_pruned.apply(get_pct_change, args=(\"2000-01-31\", \"2019-12-31\"), axis=1)\n",
    "\n",
    "# Generate mean within-year (month-to-month) variance over 2000-2019\n",
    "all_data_pruned[\"ZHVI_vol\"] = all_data_pruned.apply(get_volatility, args=(housing_vars,), axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary variables\n",
    "all_data_pruned = all_data_pruned.drop(columns=housing_vars)\n",
    "\n",
    "# Save data after cleaning\n",
    "all_data_pruned.to_csv(path + \"Data\\\\all_data_pruned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Descriptive Statistics\n",
    "\n",
    "Note: After running the above code, we may always start here since the pre-processing steps are saved in the all_data_pruned.csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State Name</th>\n",
       "      <th>County Name</th>\n",
       "      <th>state_fips</th>\n",
       "      <th>county_fips</th>\n",
       "      <th>Avalanche</th>\n",
       "      <th>Coastal Flooding</th>\n",
       "      <th>Cold Wave</th>\n",
       "      <th>Drought</th>\n",
       "      <th>Earthquake</th>\n",
       "      <th>Hail</th>\n",
       "      <th>...</th>\n",
       "      <th>share_black2000</th>\n",
       "      <th>share_hisp2000</th>\n",
       "      <th>share_asian2000</th>\n",
       "      <th>share_white2010</th>\n",
       "      <th>share_black2010</th>\n",
       "      <th>share_hisp2010</th>\n",
       "      <th>share_asian2010</th>\n",
       "      <th>ZHVI_mean</th>\n",
       "      <th>ZHVI_trend</th>\n",
       "      <th>ZHVI_vol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>California</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>6</td>\n",
       "      <td>37</td>\n",
       "      <td>33.653846</td>\n",
       "      <td>43.259557</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>73.846643</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>48.106904</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100307</td>\n",
       "      <td>0.439193</td>\n",
       "      <td>0.105664</td>\n",
       "      <td>0.277873</td>\n",
       "      <td>0.089271</td>\n",
       "      <td>0.477450</td>\n",
       "      <td>0.121261</td>\n",
       "      <td>421124.410815</td>\n",
       "      <td>2.057027</td>\n",
       "      <td>0.034877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Illinois</td>\n",
       "      <td>Cook</td>\n",
       "      <td>17</td>\n",
       "      <td>31</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>44.265594</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>19.949093</td>\n",
       "      <td>96.659243</td>\n",
       "      <td>93.923003</td>\n",
       "      <td>...</td>\n",
       "      <td>0.242277</td>\n",
       "      <td>0.196171</td>\n",
       "      <td>0.046331</td>\n",
       "      <td>0.438595</td>\n",
       "      <td>0.249698</td>\n",
       "      <td>0.239623</td>\n",
       "      <td>0.056553</td>\n",
       "      <td>197042.801648</td>\n",
       "      <td>0.587883</td>\n",
       "      <td>0.020780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Texas</td>\n",
       "      <td>Harris</td>\n",
       "      <td>48</td>\n",
       "      <td>201</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>73.843058</td>\n",
       "      <td>99.204582</td>\n",
       "      <td>88.450525</td>\n",
       "      <td>90.741330</td>\n",
       "      <td>94.050270</td>\n",
       "      <td>...</td>\n",
       "      <td>0.176150</td>\n",
       "      <td>0.307376</td>\n",
       "      <td>0.045832</td>\n",
       "      <td>0.329789</td>\n",
       "      <td>0.189457</td>\n",
       "      <td>0.408444</td>\n",
       "      <td>0.051969</td>\n",
       "      <td>140117.773455</td>\n",
       "      <td>0.856429</td>\n",
       "      <td>0.013150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arizona</td>\n",
       "      <td>Maricopa</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>85.841553</td>\n",
       "      <td>98.218263</td>\n",
       "      <td>99.331849</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037874</td>\n",
       "      <td>0.243300</td>\n",
       "      <td>0.017987</td>\n",
       "      <td>0.586845</td>\n",
       "      <td>0.054242</td>\n",
       "      <td>0.295705</td>\n",
       "      <td>0.030152</td>\n",
       "      <td>206654.847968</td>\n",
       "      <td>1.082801</td>\n",
       "      <td>0.034382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>California</td>\n",
       "      <td>San Diego</td>\n",
       "      <td>6</td>\n",
       "      <td>73</td>\n",
       "      <td>31.250000</td>\n",
       "      <td>32.796781</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>89.564111</td>\n",
       "      <td>99.745466</td>\n",
       "      <td>22.589882</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060665</td>\n",
       "      <td>0.260778</td>\n",
       "      <td>0.083679</td>\n",
       "      <td>0.484619</td>\n",
       "      <td>0.055997</td>\n",
       "      <td>0.320274</td>\n",
       "      <td>0.098978</td>\n",
       "      <td>424881.784092</td>\n",
       "      <td>1.785324</td>\n",
       "      <td>0.030891</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 52 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   State Name  County Name  state_fips  county_fips  Avalanche  \\\n",
       "0  California  Los Angeles           6           37  33.653846   \n",
       "1    Illinois         Cook          17           31   0.000000   \n",
       "2       Texas       Harris          48          201   0.000000   \n",
       "3     Arizona     Maricopa           4           13   0.000000   \n",
       "4  California    San Diego           6           73  31.250000   \n",
       "\n",
       "   Coastal Flooding   Cold Wave    Drought  Earthquake       Hail  ...  \\\n",
       "0         43.259557    0.000000  73.846643  100.000000  48.106904  ...   \n",
       "1         44.265594  100.000000  19.949093   96.659243  93.923003  ...   \n",
       "2         73.843058   99.204582  88.450525   90.741330  94.050270  ...   \n",
       "3          0.000000    0.000000  85.841553   98.218263  99.331849  ...   \n",
       "4         32.796781    0.000000  89.564111   99.745466  22.589882  ...   \n",
       "\n",
       "   share_black2000  share_hisp2000  share_asian2000  share_white2010  \\\n",
       "0         0.100307        0.439193         0.105664         0.277873   \n",
       "1         0.242277        0.196171         0.046331         0.438595   \n",
       "2         0.176150        0.307376         0.045832         0.329789   \n",
       "3         0.037874        0.243300         0.017987         0.586845   \n",
       "4         0.060665        0.260778         0.083679         0.484619   \n",
       "\n",
       "   share_black2010  share_hisp2010  share_asian2010      ZHVI_mean  \\\n",
       "0         0.089271        0.477450         0.121261  421124.410815   \n",
       "1         0.249698        0.239623         0.056553  197042.801648   \n",
       "2         0.189457        0.408444         0.051969  140117.773455   \n",
       "3         0.054242        0.295705         0.030152  206654.847968   \n",
       "4         0.055997        0.320274         0.098978  424881.784092   \n",
       "\n",
       "   ZHVI_trend  ZHVI_vol  \n",
       "0    2.057027  0.034877  \n",
       "1    0.587883  0.020780  \n",
       "2    0.856429  0.013150  \n",
       "3    1.082801  0.034382  \n",
       "4    1.785324  0.030891  \n",
       "\n",
       "[5 rows x 52 columns]"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "working_data = pd.read_csv(path + \"Data\\\\all_data_pruned.csv\")\n",
    "working_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate map of hazard risk\n",
    "\n",
    "# Generate map of housing prices\n",
    "\n",
    "# Generate map of housing price trend\n",
    "\n",
    "# Generate map of housing price volatility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state_fips</th>\n",
       "      <th>county_fips</th>\n",
       "      <th>Avalanche</th>\n",
       "      <th>Coastal Flooding</th>\n",
       "      <th>Cold Wave</th>\n",
       "      <th>Drought</th>\n",
       "      <th>Earthquake</th>\n",
       "      <th>Hail</th>\n",
       "      <th>Heat Wave</th>\n",
       "      <th>Hurricane</th>\n",
       "      <th>...</th>\n",
       "      <th>share_black2000</th>\n",
       "      <th>share_hisp2000</th>\n",
       "      <th>share_asian2000</th>\n",
       "      <th>share_white2010</th>\n",
       "      <th>share_black2010</th>\n",
       "      <th>share_hisp2010</th>\n",
       "      <th>share_asian2010</th>\n",
       "      <th>ZHVI_mean</th>\n",
       "      <th>ZHVI_trend</th>\n",
       "      <th>ZHVI_vol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3073.000000</td>\n",
       "      <td>3073.000000</td>\n",
       "      <td>3073.000000</td>\n",
       "      <td>3073.000000</td>\n",
       "      <td>3073.000000</td>\n",
       "      <td>3073.000000</td>\n",
       "      <td>3073.000000</td>\n",
       "      <td>3073.000000</td>\n",
       "      <td>3073.000000</td>\n",
       "      <td>3073.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>3073.000000</td>\n",
       "      <td>3073.000000</td>\n",
       "      <td>3050.000000</td>\n",
       "      <td>3073.000000</td>\n",
       "      <td>3073.000000</td>\n",
       "      <td>3073.000000</td>\n",
       "      <td>3059.000000</td>\n",
       "      <td>3005.000000</td>\n",
       "      <td>1034.000000</td>\n",
       "      <td>2988.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>30.279857</td>\n",
       "      <td>103.366417</td>\n",
       "      <td>3.230056</td>\n",
       "      <td>7.867445</td>\n",
       "      <td>46.351206</td>\n",
       "      <td>49.280821</td>\n",
       "      <td>50.595847</td>\n",
       "      <td>50.327553</td>\n",
       "      <td>49.254025</td>\n",
       "      <td>36.173363</td>\n",
       "      <td>...</td>\n",
       "      <td>0.088648</td>\n",
       "      <td>0.060585</td>\n",
       "      <td>0.007259</td>\n",
       "      <td>0.786937</td>\n",
       "      <td>0.094079</td>\n",
       "      <td>0.082362</td>\n",
       "      <td>0.009541</td>\n",
       "      <td>138742.559884</td>\n",
       "      <td>0.895186</td>\n",
       "      <td>0.016478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>15.063746</td>\n",
       "      <td>107.779895</td>\n",
       "      <td>14.412471</td>\n",
       "      <td>21.809221</td>\n",
       "      <td>33.456454</td>\n",
       "      <td>30.498124</td>\n",
       "      <td>28.676059</td>\n",
       "      <td>28.770805</td>\n",
       "      <td>30.869760</td>\n",
       "      <td>33.256354</td>\n",
       "      <td>...</td>\n",
       "      <td>0.142850</td>\n",
       "      <td>0.116709</td>\n",
       "      <td>0.019129</td>\n",
       "      <td>0.192676</td>\n",
       "      <td>0.145760</td>\n",
       "      <td>0.129940</td>\n",
       "      <td>0.021505</td>\n",
       "      <td>80068.204022</td>\n",
       "      <td>0.418654</td>\n",
       "      <td>0.006075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000820</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028604</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001311</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24942.839331</td>\n",
       "      <td>-0.134327</td>\n",
       "      <td>0.002858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>19.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.167038</td>\n",
       "      <td>25.930640</td>\n",
       "      <td>25.389755</td>\n",
       "      <td>26.057906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004115</td>\n",
       "      <td>0.009270</td>\n",
       "      <td>0.001705</td>\n",
       "      <td>0.673463</td>\n",
       "      <td>0.007189</td>\n",
       "      <td>0.016089</td>\n",
       "      <td>0.002425</td>\n",
       "      <td>91076.527040</td>\n",
       "      <td>0.598596</td>\n",
       "      <td>0.012313</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        state_fips  county_fips    Avalanche  Coastal Flooding    Cold Wave  \\\n",
       "count  3073.000000  3073.000000  3073.000000       3073.000000  3073.000000   \n",
       "mean     30.279857   103.366417     3.230056          7.867445    46.351206   \n",
       "std      15.063746   107.779895    14.412471         21.809221    33.456454   \n",
       "min       1.000000     1.000000     0.000000          0.000000     0.000000   \n",
       "25%      19.000000    35.000000     0.000000          0.000000     0.000000   \n",
       "\n",
       "           Drought   Earthquake         Hail    Heat Wave    Hurricane  ...  \\\n",
       "count  3073.000000  3073.000000  3073.000000  3073.000000  3073.000000  ...   \n",
       "mean     49.280821    50.595847    50.327553    49.254025    36.173363  ...   \n",
       "std      30.498124    28.676059    28.770805    30.869760    33.256354  ...   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
       "25%      25.167038    25.930640    25.389755    26.057906     0.000000  ...   \n",
       "\n",
       "       share_black2000  share_hisp2000  share_asian2000  share_white2010  \\\n",
       "count      3073.000000     3073.000000      3050.000000      3073.000000   \n",
       "mean          0.088648        0.060585         0.007259         0.786937   \n",
       "std           0.142850        0.116709         0.019129         0.192676   \n",
       "min           0.000000        0.000820         0.000000         0.028604   \n",
       "25%           0.004115        0.009270         0.001705         0.673463   \n",
       "\n",
       "       share_black2010  share_hisp2010  share_asian2010      ZHVI_mean  \\\n",
       "count      3073.000000     3073.000000      3059.000000    3005.000000   \n",
       "mean          0.094079        0.082362         0.009541  138742.559884   \n",
       "std           0.145760        0.129940         0.021505   80068.204022   \n",
       "min           0.000000        0.001311         0.000000   24942.839331   \n",
       "25%           0.007189        0.016089         0.002425   91076.527040   \n",
       "\n",
       "        ZHVI_trend     ZHVI_vol  \n",
       "count  1034.000000  2988.000000  \n",
       "mean      0.895186     0.016478  \n",
       "std       0.418654     0.006075  \n",
       "min      -0.134327     0.002858  \n",
       "25%       0.598596     0.012313  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write descriptive statistics to logfile\n",
    "summary_statistics = working_data.describe()\n",
    "summary_statistics.to_csv(path + '\\\\Figures\\\\summary_statistics.csv', index_label='Metric')\n",
    "summary_statistics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Regressions\n",
    "\n",
    "We use standard OLS estimation, LASSO, and Ridge regressions to capture both what variables are the most correlated and predictive of long-term trends, average magnitude, and short-term volatility in local housing prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lists of independent (hr), dependent (y), and control (X) variables for analysis\n",
    "hr_vars = [hazard_rename_dict[i] for i in hazard_vars]\n",
    "y_vars = [\"ZHVI_mean\", \"ZHVI_trend\", \"ZHVI_vol\"]\n",
    "X_vars = control_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OLS\n",
    "\n",
    "We use this approach as a baseline estimator of the relationships between hazard risk and housing price characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct OLS Regression on mean ZHVI (without controls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct OLS Regression on mean ZHVI (with controls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct OLS Regression on change in ZHVI (without controls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct OLS Regression on change in ZHVI (with controls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct OLS Regression on mean yearly variance in ZHVI (without controls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct OLS Regression on mean yearly variance in ZHVI (with controls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LASSO\n",
    "\n",
    "We conduct this analysis to assess variable selection and dimension reduction, particularly in the context of what hazards are important in predicting ZHVI characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct LASSO Regression on mean ZHVI (without controls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct LASSO Regression on mean ZHVI (with controls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct LASSO Regression on change in ZHVI (without controls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct LASSO Regression on change in ZHVI (with controls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct LASSO Regression on mean yearly variance in ZHVI (without controls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct LASSO Regression on mean yearly variance in ZHVI (with controls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge\n",
    "\n",
    "We conduct this analysis for the same reason as LASSO: to assess variable selection and dimension reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct Ridge Regression on mean ZHVI (without controls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct Ridge Regression on mean ZHVI (with controls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct Ridge Regression on change in ZHVI (without controls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct Ridge Regression on change in ZHVI (with controls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct Ridge Regression on mean yearly variance in ZHVI (without controls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct Ridge Regression on mean yearly variance in ZHVI (with controls)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlta",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
